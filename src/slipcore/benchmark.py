"""
Slipstream Token Efficiency Benchmark

Measures actual token usage with real BPE tokenizers (cl100k_base / o200k_base).
Validates the claimed 82% token reduction for SLIP protocol.

Usage:
    python -m slipcore.benchmark
    python -m slipcore.benchmark --tokenizer o200k_base
    python -m slipcore.benchmark --verbose
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Tuple
import json
import statistics

# Lazy import tiktoken
_tiktoken = None


def _get_tokenizer(name: str = "cl100k_base"):
    """Get tiktoken encoder, installing if needed."""
    global _tiktoken
    if _tiktoken is None:
        try:
            import tiktoken
            _tiktoken = tiktoken
        except ImportError:
            raise ImportError(
                "tiktoken is required for benchmarking: pip install tiktoken"
            )
    return _tiktoken.get_encoding(name)


# ============ Test Cases ============

# Realistic agent-to-agent message pairs: (natural_language, slip_wire)
BENCHMARK_CASES: List[Tuple[str, str]] = [
    # Task coordination
    (
        "Please review the authentication module for security vulnerabilities",
        "SLIP v1 dev reviewer RequestReview auth_module",
    ),
    (
        "I have completed the implementation of the user login feature",
        "SLIP v1 exec coord InformComplete login_feature",
    ),
    (
        "Can you clarify the requirements for the API rate limiting?",
        "SLIP v1 dev pm AskClarify rate_limiting",
    ),
    (
        "Request: Execute the database migration script on staging",
        "SLIP v1 coord ops RequestTask db_migrate staging",
    ),
    (
        "Proposal: We should refactor the payment processing module to use async/await",
        "SLIP v1 arch team ProposePlan payment_refactor",
    ),

    # Status updates
    (
        "Status update: Currently working on the caching layer, 60% complete",
        "SLIP v1 dev coord InformProgress caching 60pct",
    ),
    (
        "I am blocked waiting for the API credentials from the security team",
        "SLIP v1 dev coord InformBlocked api_creds",
    ),
    (
        "The test suite is passing, all 127 tests green",
        "SLIP v1 ci coord InformResult tests_pass 127",
    ),

    # Reviews and approvals
    (
        "Code review: Approved, the implementation looks good and follows best practices",
        "SLIP v1 reviewer dev EvalApprove",
    ),
    (
        "Review feedback: Needs work, please add error handling for network failures",
        "SLIP v1 reviewer dev EvalNeedsWork error_handling",
    ),
    (
        "Decision: Reject this approach, it introduces too much complexity",
        "SLIP v1 arch dev Reject complexity",
    ),

    # Error handling
    (
        "Error: The build failed due to missing dependency on the Redis client library",
        "SLIP v1 ci coord ErrorResource redis_dep",
    ),
    (
        "Alert: Database connection timeout after 30 seconds, retrying",
        "SLIP v1 monitor coord ObserveError db_timeout",
    ),
    (
        "Exception: Permission denied when accessing the configuration file",
        "SLIP v1 app ops ErrorPermission config_file",
    ),

    # Meta coordination
    (
        "Acknowledged, I received your request and will begin processing",
        "SLIP v1 exec coord MetaAck",
    ),
    (
        "Escalating this issue to the senior engineering team for review",
        "SLIP v1 coord lead MetaEscalate",
    ),
    (
        "Handing off the deployment responsibility to the on-call engineer",
        "SLIP v1 ops oncall MetaHandoff deploy",
    ),

    # Complex scenarios
    (
        "I propose we implement a caching layer using Redis with a 5-minute TTL for API responses to reduce database load",
        "SLIP v1 arch team ProposePlan redis_cache 5min_ttl",
    ),
    (
        "Request for assistance: I need help debugging the OAuth2 callback handler, authentication is failing with invalid_grant error",
        "SLIP v1 dev senior RequestHelp oauth_debug invalid_grant",
    ),
    (
        "Observation: The CPU usage on prod-server-3 has spiked to 95% after the latest deployment",
        "SLIP v1 monitor ops ObserveChange cpu_spike prod3 95pct",
    ),
]


# ============ Benchmark Results ============

@dataclass
class TokenStats:
    """Statistics for a single test case."""
    natural_tokens: int
    slip_tokens: int
    reduction: float  # percentage reduction
    natural_text: str
    slip_wire: str


@dataclass
class BenchmarkResults:
    """Aggregate benchmark results."""
    tokenizer_name: str
    total_cases: int
    natural_tokens_total: int = 0
    slip_tokens_total: int = 0
    avg_reduction: float = 0.0
    min_reduction: float = 0.0
    max_reduction: float = 0.0
    median_reduction: float = 0.0
    individual_stats: List[TokenStats] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "tokenizer": self.tokenizer_name,
            "total_cases": self.total_cases,
            "natural_tokens_total": self.natural_tokens_total,
            "slip_tokens_total": self.slip_tokens_total,
            "overall_reduction": round(
                (1 - self.slip_tokens_total / self.natural_tokens_total) * 100, 1
            ) if self.natural_tokens_total > 0 else 0,
            "avg_reduction_pct": round(self.avg_reduction, 1),
            "min_reduction_pct": round(self.min_reduction, 1),
            "max_reduction_pct": round(self.max_reduction, 1),
            "median_reduction_pct": round(self.median_reduction, 1),
        }


# ============ Benchmark Functions ============

def count_tokens(text: str, encoder) -> int:
    """Count tokens in text using the given encoder."""
    return len(encoder.encode(text))


def analyze_tokenization(text: str, encoder, label: str = "") -> None:
    """Print detailed tokenization analysis for debugging."""
    tokens = encoder.encode(text)
    decoded = [encoder.decode([t]) for t in tokens]
    print(f"\n{label} ({len(tokens)} tokens):")
    print(f"  Text: {text}")
    print(f"  Tokens: {decoded}")


def run_benchmark(
    cases: Optional[List[Tuple[str, str]]] = None,
    tokenizer_name: str = "cl100k_base",
    verbose: bool = False,
) -> BenchmarkResults:
    """
    Run token efficiency benchmark.

    Args:
        cases: List of (natural_language, slip_wire) pairs
        tokenizer_name: tiktoken encoding name
        verbose: Print detailed analysis

    Returns:
        BenchmarkResults with statistics
    """
    if cases is None:
        cases = BENCHMARK_CASES

    encoder = _get_tokenizer(tokenizer_name)
    results = BenchmarkResults(
        tokenizer_name=tokenizer_name,
        total_cases=len(cases),
    )

    reductions: List[float] = []

    print(f"\n{'='*70}")
    print(f"SLIPSTREAM TOKEN EFFICIENCY BENCHMARK")
    print(f"Tokenizer: {tokenizer_name}")
    print(f"Test cases: {len(cases)}")
    print(f"{'='*70}\n")

    for i, (natural, slip) in enumerate(cases, 1):
        nat_tokens = count_tokens(natural, encoder)
        slip_tokens = count_tokens(slip, encoder)
        reduction = (1 - slip_tokens / nat_tokens) * 100 if nat_tokens > 0 else 0

        stats = TokenStats(
            natural_tokens=nat_tokens,
            slip_tokens=slip_tokens,
            reduction=reduction,
            natural_text=natural,
            slip_wire=slip,
        )
        results.individual_stats.append(stats)
        results.natural_tokens_total += nat_tokens
        results.slip_tokens_total += slip_tokens
        reductions.append(reduction)

        if verbose:
            print(f"Case {i:2d}:")
            print(f"  Natural ({nat_tokens:2d} tok): {natural[:60]}...")
            print(f"  SLIP    ({slip_tokens:2d} tok): {slip}")
            print(f"  Reduction: {reduction:.1f}%\n")

    # Calculate aggregate stats
    results.avg_reduction = statistics.mean(reductions) if reductions else 0
    results.min_reduction = min(reductions) if reductions else 0
    results.max_reduction = max(reductions) if reductions else 0
    results.median_reduction = statistics.median(reductions) if reductions else 0

    # Print summary
    overall = (1 - results.slip_tokens_total / results.natural_tokens_total) * 100

    print(f"\n{'='*70}")
    print("BENCHMARK RESULTS")
    print(f"{'='*70}")
    print(f"Total natural language tokens: {results.natural_tokens_total}")
    print(f"Total SLIP wire tokens:        {results.slip_tokens_total}")
    print(f"Overall reduction:             {overall:.1f}%")
    print(f"{'='*70}")
    print(f"Per-case statistics:")
    print(f"  Average reduction:  {results.avg_reduction:.1f}%")
    print(f"  Median reduction:   {results.median_reduction:.1f}%")
    print(f"  Min reduction:      {results.min_reduction:.1f}%")
    print(f"  Max reduction:      {results.max_reduction:.1f}%")
    print(f"{'='*70}\n")

    # Tokenization detail for key examples
    if verbose:
        print("\nTOKENIZATION DETAIL (sample cases):\n")
        for case in [BENCHMARK_CASES[0], BENCHMARK_CASES[3], BENCHMARK_CASES[-1]]:
            analyze_tokenization(case[0], encoder, "Natural")
            analyze_tokenization(case[1], encoder, "SLIP")
            print()

    return results


def compare_formats(text: str, tokenizer_name: str = "cl100k_base") -> None:
    """
    Compare token counts for different wire format approaches.

    Shows why SLIP's mnemonic format beats:
    - Hex indices like [0x0011]
    - Arrow notation like [7â†’3:REQ_EVAL]
    - JSON formats
    """
    encoder = _get_tokenizer(tokenizer_name)

    # Example: "Please review the auth code for security issues"
    formats = {
        "Natural language": text,
        "SLIP v1 (mnemonic)": "SLIP v1 dev reviewer RequestReview auth_security",
        "Hex index format": "[0x0032] src=dev dst=reviewer payload=auth_security",
        "Arrow notation": "[7->3:REQ_REVIEW_SECURITY] dev->reviewer auth",
        "JSON compact": '{"act":"RequestReview","src":"dev","dst":"reviewer","p":"auth"}',
        "JSON full": '{"action":"RequestReview","source":"dev","destination":"reviewer","payload":"auth_security"}',
    }

    print(f"\n{'='*70}")
    print("WIRE FORMAT COMPARISON")
    print(f"Tokenizer: {tokenizer_name}")
    print(f"{'='*70}\n")

    for name, wire in formats.items():
        tokens = count_tokens(wire, encoder)
        print(f"{name:25s} ({tokens:2d} tok): {wire}")

    print(f"\n{'='*70}")
    print("KEY INSIGHT: Special characters (arrows, brackets, colons) fragment")
    print("into multiple BPE tokens. SLIP is 40-50% better than hex/JSON.")
    print("For short messages, natural language may be more efficient,")
    print("but SLIP provides semantic consistency and parsability.")
    print(f"{'='*70}\n")


def benchmark_mnemonic_efficiency(tokenizer_name: str = "cl100k_base") -> None:
    """
    Show token counts for individual mnemonics vs alternatives.
    """
    encoder = _get_tokenizer(tokenizer_name)

    comparisons = [
        # (mnemonic, hex_id, description)
        ("RequestReview", "0x0032", "Request code review"),
        ("InformComplete", "0x0012", "Task completed"),
        ("EvalApprove", "0x0060", "Evaluation approved"),
        ("MetaEscalate", "0x0073", "Escalate issue"),
        ("AskClarify", "0x0020", "Ask for clarification"),
        ("ProposePlan", "0x0040", "Propose a plan"),
    ]

    print(f"\n{'='*60}")
    print("MNEMONIC vs HEX TOKEN EFFICIENCY")
    print(f"{'='*60}\n")
    print(f"{'Mnemonic':<18} {'Tok':>4}  {'Hex ID':<8} {'Tok':>4}  {'Savings':>8}")
    print("-" * 60)

    for mnemonic, hex_id, desc in comparisons:
        m_tok = count_tokens(mnemonic, encoder)
        h_tok = count_tokens(hex_id, encoder)
        savings = h_tok - m_tok
        sign = "+" if savings > 0 else ""
        print(f"{mnemonic:<18} {m_tok:>4}  {hex_id:<8} {h_tok:>4}  {sign}{savings:>7}")

    print("-" * 60)
    print("Note: Hex IDs often tokenize as 3-4 tokens (0x + digits)")
    print("Mnemonics like 'RequestReview' tokenize as 1-2 tokens")
    print(f"{'='*60}\n")


# ============ CLI ============

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Slipstream token efficiency benchmark"
    )
    parser.add_argument(
        "-t", "--tokenizer",
        default="cl100k_base",
        choices=["cl100k_base", "o200k_base"],
        help="Tokenizer to use (default: cl100k_base for GPT-4)"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Show detailed tokenization"
    )
    parser.add_argument(
        "--compare-formats",
        action="store_true",
        help="Compare different wire format approaches"
    )
    parser.add_argument(
        "--mnemonic-test",
        action="store_true",
        help="Test mnemonic vs hex efficiency"
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="Output results as JSON"
    )

    args = parser.parse_args()

    if args.compare_formats:
        compare_formats(
            "Please review the authentication code for security vulnerabilities",
            args.tokenizer
        )
    elif args.mnemonic_test:
        benchmark_mnemonic_efficiency(args.tokenizer)
    else:
        results = run_benchmark(
            tokenizer_name=args.tokenizer,
            verbose=args.verbose
        )

        if args.json:
            print(json.dumps(results.to_dict(), indent=2))
